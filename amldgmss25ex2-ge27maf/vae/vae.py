import torch
import torch.nn as nn
import torch.nn.functional as F

from typeguard import typechecked
from torchtyping import TensorType, patch_typeguard
from typing import Tuple

from .decoder import Decoder
from .encoder import Encoder

patch_typeguard()


class VAE(nn.Module):
    def __init__(self, input_dim: int, latent_dim: int, hidden_dim: int = 100):
        """Initialize the VAE model.
        
        Args:
            obs_dim (int): Dimension of the observed data x, int
            latent_dim (int): Dimension of the latent variable z, int
            hidden_dim (int): Hidden dimension of the encoder/decoder networks, int
        """
        super().__init__()
        self.latent_dim = latent_dim
        self.encoder = Encoder(input_dim, latent_dim, hidden_dim=hidden_dim)
        self.decoder = Decoder(input_dim, latent_dim, hidden_dim=hidden_dim)

    @typechecked
    def sample_with_reparametrization(self, mu: TensorType['batch_size', 'latent_dim'],
                                      logsigma: TensorType['batch_size', 'latent_dim']) -> TensorType[
        'batch_size', 'latent_dim']:
        """Draw sample from q(z) with reparametrization.
        
        We draw a single sample z_i for each data point x_i.
        
        Args:
            mu: Means of q(z) for the batch, shape [batch_size, latent_dim]
            logsigma: Log-sigmas of q(z) for the batch, shape [batch_size, latent_dim]
        
        Returns:
            z: Latent variables samples from q(z), shape [batch_size, latent_dim]
        """
        std = torch.exp(logsigma)
        # sample epsilon from normal distribution
        epsilon = torch.randn_like(std)
        # reparametrization trick: z = mu + sigma * epsilon
        # randomness comes from epsilon,
        # while the path from mu and sigma to z is deterministic (and thus, differentiable).
        return mu + std * epsilon

    @typechecked
    def kl_divergence(self, mu: TensorType['batch_size', 'latent_dim'],
                      logsigma: TensorType['batch_size', 'latent_dim']) -> TensorType['batch_size']:
        """Compute KL divergence KL(q_i(z)||p(z)) for each q_i in the batch.
        
        Args:
            mu: Means of the q_i distributions, shape [batch_size, latent_dim]
            logsigma: Logarithm of standard deviations of the q_i distributions,
                      shape [batch_size, latent_dim]
        
        Returns:
            kl: KL divergence for each of the q_i distributions, shape [batch_size]
        """
        # closed-form solution for KL(N(mu, sigma^2) \| N(0, 1)) is: 0.5 * sum(mu^2 + sigma^2 - 2 * log(sigma) - 1)
        # (sum is over latent dimension)
        kl_terms = mu ** 2 + torch.exp(2 * logsigma) - 2 * logsigma - 1
        kl = 0.5 * torch.sum(kl_terms, dim=1)

        return kl

    @typechecked
    def elbo(self, x: TensorType['batch_size', 'input_dim']) -> TensorType['batch_size']:
        """Estimate the ELBO for the mini-batch of data.
        
        Args:
            x: Mini-batch of the observations, shape [batch_size, input_dim]
        
        Returns:
            elbo_mc: MC estimate of ELBO for each sample in the mini-batch, shape [batch_size]
        """
        # pass input through encoder to obtain q(z|x)
        mu, logsigma = self.encoder(x)

        # kl divergence term
        kl = self.kl_divergence(mu, logsigma)

        # sample from latent distribution q(z|x) using reparametrization (one sample to approximate the expectationn)
        z = self.sample_with_reparametrization(mu, logsigma)

        # pass latent samples through the decoder to get params of the likelihood p(|z)
        theta = self.decoder(z)

        # compute reconstruction log-likelihood term for each sample.
        # for Bernoulli likelihood, this is the negative of the binary cross-entropy loss
        reconstruction_log_likelihood = -torch.sum(F.binary_cross_entropy(theta, x, reduction='none'), dim=1)

        # calculate ELBO
        elbo_mc = reconstruction_log_likelihood - kl

        return elbo_mc


    @typechecked
    def sample(self, num_samples: int, device: str = 'cpu') -> Tuple[
        TensorType['num_samples', 'latent_dim'],
        TensorType['num_samples', 'input_dim'],
        TensorType['num_samples', 'input_dim']]:
        """Generate new samples from the model.
        
        Args:
            num_samples: Number of samples to generate.
        
        Returns:
            z: Sampled latent codes, shape [num_samples, latent_dim]
            theta: Parameters of the output distribution, shape [num_samples, input_dim]
            x: Corresponding samples generated by the model, shape [num_samples, input_dim]
        """
        self.eval()

        with torch.no_grad():
            # sample latent vectors z from prior distribution p(z) ~ N(0, I)
            z = torch.randn(num_samples, self.latent_dim).to(device)

            # pass latent vectors through decoder to get the parameters of the conditional likelihood p(x|z).
            theta = self.decoder(z)

            # sample the final data points x from the conditional likelihood.
            x = torch.bernoulli(theta)

            return z, theta, x
